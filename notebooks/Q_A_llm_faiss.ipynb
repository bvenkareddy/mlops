{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Upload PDF and the create INDEX and finally Q&A"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16416,"status":"ok","timestamp":1687583105717,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"7xQ4NrSVygd7","outputId":"ddd6d53a-e857-4a85-b573-a69f6f3e39ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain\n","  Downloading langchain-0.0.234-py3-none-any.whl (1.3 MB)\n","                                              0.0/1.3 MB ? eta -:--:--\n","     ---                                      0.1/1.3 MB 3.2 MB/s eta 0:00:01\n","     ---------------                          0.5/1.3 MB 5.2 MB/s eta 0:00:01\n","     ---------------------------------------  1.3/1.3 MB 10.3 MB/s eta 0:00:01\n","     ---------------------------------------- 1.3/1.3 MB 8.3 MB/s eta 0:00:00\n","Collecting PyYAML>=5.4.1 (from langchain)\n","  Using cached PyYAML-6.0-cp311-cp311-win_amd64.whl (143 kB)\n","Collecting SQLAlchemy<3,>=1.4 (from langchain)\n","  Downloading SQLAlchemy-2.0.19-cp311-cp311-win_amd64.whl (2.0 MB)\n","                                              0.0/2.0 MB ? eta -:--:--\n","     -----------------------------            1.5/2.0 MB 45.8 MB/s eta 0:00:01\n","     ---------------------------------------- 2.0/2.0 MB 31.4 MB/s eta 0:00:00\n","Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n","  Downloading aiohttp-3.8.4-cp311-cp311-win_amd64.whl (317 kB)\n","                                              0.0/317.2 kB ? eta -:--:--\n","     -------------------------------------- 317.2/317.2 kB 9.6 MB/s eta 0:00:00\n","Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n","Collecting langsmith<0.0.6,>=0.0.5 (from langchain)\n","  Downloading langsmith-0.0.5-py3-none-any.whl (25 kB)\n","Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n","  Downloading numexpr-2.8.4-cp311-cp311-win_amd64.whl (92 kB)\n","                                              0.0/92.7 kB ? eta -:--:--\n","     ---------------------------------------- 92.7/92.7 kB 2.7 MB/s eta 0:00:00\n","Collecting numpy<2,>=1 (from langchain)\n","  Using cached numpy-1.25.1-cp311-cp311-win_amd64.whl (15.0 MB)\n","Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n","  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n","                                              0.0/90.0 kB ? eta -:--:--\n","     ---------------------------------------- 90.0/90.0 kB 2.6 MB/s eta 0:00:00\n","Collecting pydantic<2,>=1 (from langchain)\n","  Using cached pydantic-1.10.11-cp311-cp311-win_amd64.whl (2.1 MB)\n","Collecting requests<3,>=2 (from langchain)\n","  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n","Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n","  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n","Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n","                                              0.0/61.2 kB ? eta -:--:--\n","     ---------------------------------------- 61.2/61.2 kB 1.6 MB/s eta 0:00:00\n","Collecting charset-normalizer<4.0,>=2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Using cached charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl (96 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Downloading yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)\n","                                              0.0/60.2 kB ? eta -:--:--\n","     ---------------------------------------- 60.2/60.2 kB 3.1 MB/s eta 0:00:00\n","Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl (44 kB)\n","                                              0.0/44.9 kB ? eta -:--:--\n","     ---------------------------------------- 44.9/44.9 kB 1.1 MB/s eta 0:00:00\n","Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n","                                              0.0/49.1 kB ? eta -:--:--\n","     ---------------------------------        41.0/49.1 kB ? eta -:--:--\n","     -------------------------------------- 49.1/49.1 kB 826.3 kB/s eta 0:00:00\n","Collecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n","Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting typing-extensions>=4.2.0 (from pydantic<2,>=1->langchain)\n","  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n","Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n","  Using cached idna-3.4-py3-none-any.whl (61 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n","  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n","Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n","  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n","Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n","  Downloading greenlet-2.0.2-cp311-cp311-win_amd64.whl (192 kB)\n","                                              0.0/192.5 kB ? eta -:--:--\n","     -------------------------------------- 192.5/192.5 kB 5.9 MB/s eta 0:00:00\n","Requirement already satisfied: packaging>=17.0 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: urllib3, typing-extensions, tenacity, PyYAML, numpy, mypy-extensions, multidict, marshmallow, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, typing-inspect, SQLAlchemy, requests, pydantic, numexpr, marshmallow-enum, aiosignal, openapi-schema-pydantic, langsmith, dataclasses-json, aiohttp, langchain\n","Successfully installed PyYAML-6.0 SQLAlchemy-2.0.19 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.2.0 dataclasses-json-0.5.9 frozenlist-1.4.0 greenlet-2.0.2 idna-3.4 langchain-0.0.234 langsmith-0.0.5 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 numexpr-2.8.4 numpy-1.25.1 openapi-schema-pydantic-1.2.4 pydantic-1.10.11 requests-2.31.0 tenacity-8.2.2 typing-extensions-4.7.1 typing-inspect-0.9.0 urllib3-2.0.3 yarl-1.9.2\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2\n","[notice] To update, run: C:\\Users\\theco\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install langchain"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7495,"status":"ok","timestamp":1687583145096,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"vxI-jRrJysFF","outputId":"9e68bbe5-2068-4335-9a48-aa774c593d23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.7.4-cp311-cp311-win_amd64.whl (10.8 MB)\n","                                              0.0/10.8 MB ? eta -:--:--\n","                                              0.1/10.8 MB 3.3 MB/s eta 0:00:04\n","     -                                        0.4/10.8 MB 4.5 MB/s eta 0:00:03\n","     --                                       0.6/10.8 MB 4.6 MB/s eta 0:00:03\n","     ---                                      0.8/10.8 MB 4.7 MB/s eta 0:00:03\n","     ---                                      1.1/10.8 MB 4.8 MB/s eta 0:00:03\n","     ----                                     1.3/10.8 MB 4.8 MB/s eta 0:00:02\n","     -----                                    1.4/10.8 MB 4.6 MB/s eta 0:00:03\n","     ------                                   1.6/10.8 MB 4.5 MB/s eta 0:00:03\n","     ------                                   1.8/10.8 MB 4.3 MB/s eta 0:00:03\n","     -------                                  1.9/10.8 MB 4.2 MB/s eta 0:00:03\n","     -------                                  2.0/10.8 MB 3.9 MB/s eta 0:00:03\n","     -------                                  2.1/10.8 MB 3.8 MB/s eta 0:00:03\n","     --------                                 2.2/10.8 MB 3.7 MB/s eta 0:00:03\n","     --------                                 2.3/10.8 MB 3.6 MB/s eta 0:00:03\n","     --------                                 2.4/10.8 MB 3.6 MB/s eta 0:00:03\n","     ---------                                2.6/10.8 MB 3.6 MB/s eta 0:00:03\n","     ---------                                2.7/10.8 MB 3.4 MB/s eta 0:00:03\n","     ----------                               2.8/10.8 MB 3.4 MB/s eta 0:00:03\n","     ----------                               2.8/10.8 MB 3.2 MB/s eta 0:00:03\n","     ----------                               2.9/10.8 MB 3.2 MB/s eta 0:00:03\n","     ----------                               2.9/10.8 MB 3.0 MB/s eta 0:00:03\n","     -----------                              3.0/10.8 MB 3.0 MB/s eta 0:00:03\n","     -----------                              3.0/10.8 MB 2.9 MB/s eta 0:00:03\n","     -----------                              3.1/10.8 MB 2.9 MB/s eta 0:00:03\n","     -----------                              3.1/10.8 MB 2.8 MB/s eta 0:00:03\n","     -----------                              3.2/10.8 MB 2.7 MB/s eta 0:00:03\n","     ------------                             3.2/10.8 MB 2.6 MB/s eta 0:00:03\n","     ------------                             3.3/10.8 MB 2.6 MB/s eta 0:00:03\n","     ------------                             3.3/10.8 MB 2.5 MB/s eta 0:00:03\n","     ------------                             3.4/10.8 MB 2.5 MB/s eta 0:00:04\n","     ------------                             3.5/10.8 MB 2.4 MB/s eta 0:00:04\n","     -------------                            3.5/10.8 MB 2.4 MB/s eta 0:00:04\n","     -------------                            3.6/10.8 MB 2.4 MB/s eta 0:00:04\n","     -------------                            3.6/10.8 MB 2.3 MB/s eta 0:00:04\n","     -------------                            3.7/10.8 MB 2.3 MB/s eta 0:00:04\n","     --------------                           3.8/10.8 MB 2.3 MB/s eta 0:00:04\n","     --------------                           3.9/10.8 MB 2.3 MB/s eta 0:00:04\n","     --------------                           3.9/10.8 MB 2.3 MB/s eta 0:00:04\n","     --------------                           4.0/10.8 MB 2.2 MB/s eta 0:00:04\n","     ---------------                          4.0/10.8 MB 2.2 MB/s eta 0:00:04\n","     ---------------                          4.1/10.8 MB 2.2 MB/s eta 0:00:04\n","     ---------------                          4.1/10.8 MB 2.2 MB/s eta 0:00:04\n","     ---------------                          4.2/10.8 MB 2.1 MB/s eta 0:00:04\n","     ---------------                          4.2/10.8 MB 2.1 MB/s eta 0:00:04\n","     ---------------                          4.3/10.8 MB 2.1 MB/s eta 0:00:04\n","     ----------------                         4.3/10.8 MB 2.1 MB/s eta 0:00:04\n","     ----------------                         4.4/10.8 MB 2.0 MB/s eta 0:00:04\n","     ----------------                         4.4/10.8 MB 2.0 MB/s eta 0:00:04\n","     ----------------                         4.5/10.8 MB 2.0 MB/s eta 0:00:04\n","     ----------------                         4.6/10.8 MB 2.0 MB/s eta 0:00:04\n","     -----------------                        4.6/10.8 MB 2.0 MB/s eta 0:00:04\n","     -----------------                        4.7/10.8 MB 1.9 MB/s eta 0:00:04\n","     -----------------                        4.7/10.8 MB 1.9 MB/s eta 0:00:04\n","     -----------------                        4.8/10.8 MB 1.9 MB/s eta 0:00:04\n","     -----------------                        4.8/10.8 MB 1.9 MB/s eta 0:00:04\n","     ------------------                       4.9/10.8 MB 1.9 MB/s eta 0:00:04\n","     ------------------                       4.9/10.8 MB 1.9 MB/s eta 0:00:04\n","     ------------------                       5.0/10.8 MB 1.9 MB/s eta 0:00:04\n","     ------------------                       5.0/10.8 MB 1.9 MB/s eta 0:00:04\n","     ------------------                       5.1/10.8 MB 1.8 MB/s eta 0:00:04\n","     -------------------                      5.1/10.8 MB 1.8 MB/s eta 0:00:04\n","     -------------------                      5.2/10.8 MB 1.8 MB/s eta 0:00:04\n","     -------------------                      5.2/10.8 MB 1.8 MB/s eta 0:00:04\n","     -------------------                      5.3/10.8 MB 1.8 MB/s eta 0:00:04\n","     -------------------                      5.3/10.8 MB 1.8 MB/s eta 0:00:04\n","     --------------------                     5.4/10.8 MB 1.8 MB/s eta 0:00:04\n","     --------------------                     5.5/10.8 MB 1.8 MB/s eta 0:00:03\n","     --------------------                     5.5/10.8 MB 1.8 MB/s eta 0:00:03\n","     --------------------                     5.6/10.8 MB 1.8 MB/s eta 0:00:03\n","     ---------------------                    5.7/10.8 MB 1.8 MB/s eta 0:00:03\n","     ---------------------                    5.8/10.8 MB 1.8 MB/s eta 0:00:03\n","     ---------------------                    5.8/10.8 MB 1.8 MB/s eta 0:00:03\n","     ----------------------                   5.9/10.8 MB 1.8 MB/s eta 0:00:03\n","     ----------------------                   6.0/10.8 MB 1.8 MB/s eta 0:00:03\n","     ----------------------                   6.1/10.8 MB 1.8 MB/s eta 0:00:03\n","     ----------------------                   6.1/10.8 MB 1.7 MB/s eta 0:00:03\n","     ----------------------                   6.2/10.8 MB 1.7 MB/s eta 0:00:03\n","     -----------------------                  6.3/10.8 MB 1.7 MB/s eta 0:00:03\n","     -----------------------                  6.3/10.8 MB 1.7 MB/s eta 0:00:03\n","     -----------------------                  6.4/10.8 MB 1.7 MB/s eta 0:00:03\n","     ------------------------                 6.5/10.8 MB 1.7 MB/s eta 0:00:03\n","     ------------------------                 6.6/10.8 MB 1.7 MB/s eta 0:00:03\n","     ------------------------                 6.6/10.8 MB 1.7 MB/s eta 0:00:03\n","     ------------------------                 6.7/10.8 MB 1.7 MB/s eta 0:00:03\n","     -------------------------                6.8/10.8 MB 1.7 MB/s eta 0:00:03\n","     -------------------------                6.9/10.8 MB 1.7 MB/s eta 0:00:03\n","     -------------------------                6.9/10.8 MB 1.7 MB/s eta 0:00:03\n","     -------------------------                7.0/10.8 MB 1.7 MB/s eta 0:00:03\n","     --------------------------               7.1/10.8 MB 1.7 MB/s eta 0:00:03\n","     --------------------------               7.1/10.8 MB 1.7 MB/s eta 0:00:03\n","     --------------------------               7.2/10.8 MB 1.7 MB/s eta 0:00:03\n","     --------------------------               7.2/10.8 MB 1.7 MB/s eta 0:00:03\n","     ---------------------------              7.3/10.8 MB 1.7 MB/s eta 0:00:03\n","     ---------------------------              7.4/10.8 MB 1.7 MB/s eta 0:00:02\n","     ---------------------------              7.5/10.8 MB 1.7 MB/s eta 0:00:02\n","     ---------------------------              7.5/10.8 MB 1.7 MB/s eta 0:00:02\n","     ----------------------------             7.6/10.8 MB 1.7 MB/s eta 0:00:02\n","     ----------------------------             7.6/10.8 MB 1.7 MB/s eta 0:00:02\n","     ----------------------------             7.7/10.8 MB 1.7 MB/s eta 0:00:02\n","     ----------------------------             7.8/10.8 MB 1.7 MB/s eta 0:00:02\n","     -----------------------------            7.9/10.8 MB 1.7 MB/s eta 0:00:02\n","     -----------------------------            8.0/10.8 MB 1.7 MB/s eta 0:00:02\n","     -----------------------------            8.0/10.8 MB 1.7 MB/s eta 0:00:02\n","     ------------------------------           8.1/10.8 MB 1.7 MB/s eta 0:00:02\n","     ------------------------------           8.2/10.8 MB 1.7 MB/s eta 0:00:02\n","     ------------------------------           8.2/10.8 MB 1.7 MB/s eta 0:00:02\n","     ------------------------------           8.3/10.8 MB 1.7 MB/s eta 0:00:02\n","     -------------------------------          8.4/10.8 MB 1.7 MB/s eta 0:00:02\n","     -------------------------------          8.4/10.8 MB 1.7 MB/s eta 0:00:02\n","     -------------------------------          8.6/10.8 MB 1.7 MB/s eta 0:00:02\n","     --------------------------------         8.6/10.8 MB 1.7 MB/s eta 0:00:02\n","     --------------------------------         8.7/10.8 MB 1.7 MB/s eta 0:00:02\n","     --------------------------------         8.8/10.8 MB 1.7 MB/s eta 0:00:02\n","     --------------------------------         8.9/10.8 MB 1.7 MB/s eta 0:00:02\n","     ---------------------------------        9.0/10.8 MB 1.7 MB/s eta 0:00:02\n","     ---------------------------------        9.1/10.8 MB 1.7 MB/s eta 0:00:02\n","     ---------------------------------        9.2/10.8 MB 1.7 MB/s eta 0:00:01\n","     ----------------------------------       9.2/10.8 MB 1.7 MB/s eta 0:00:01\n","     ----------------------------------       9.3/10.8 MB 1.7 MB/s eta 0:00:01\n","     ----------------------------------       9.4/10.8 MB 1.7 MB/s eta 0:00:01\n","     -----------------------------------      9.5/10.8 MB 1.7 MB/s eta 0:00:01\n","     -----------------------------------      9.5/10.8 MB 1.7 MB/s eta 0:00:01\n","     -----------------------------------      9.6/10.8 MB 1.7 MB/s eta 0:00:01\n","     -----------------------------------      9.6/10.8 MB 1.7 MB/s eta 0:00:01\n","     ------------------------------------     9.7/10.8 MB 1.7 MB/s eta 0:00:01\n","     ------------------------------------     9.8/10.8 MB 1.7 MB/s eta 0:00:01\n","     ------------------------------------     9.8/10.8 MB 1.7 MB/s eta 0:00:01\n","     ------------------------------------     9.9/10.8 MB 1.7 MB/s eta 0:00:01\n","     ------------------------------------     9.9/10.8 MB 1.7 MB/s eta 0:00:01\n","     -------------------------------------    10.0/10.8 MB 1.7 MB/s eta 0:00:01\n","     -------------------------------------    10.1/10.8 MB 1.7 MB/s eta 0:00:01\n","     -------------------------------------    10.1/10.8 MB 1.6 MB/s eta 0:00:01\n","     -------------------------------------    10.2/10.8 MB 1.6 MB/s eta 0:00:01\n","     --------------------------------------   10.2/10.8 MB 1.6 MB/s eta 0:00:01\n","     --------------------------------------   10.3/10.8 MB 1.6 MB/s eta 0:00:01\n","     --------------------------------------   10.4/10.8 MB 1.6 MB/s eta 0:00:01\n","     --------------------------------------   10.4/10.8 MB 1.6 MB/s eta 0:00:01\n","     --------------------------------------   10.5/10.8 MB 1.6 MB/s eta 0:00:01\n","     ---------------------------------------  10.6/10.8 MB 1.6 MB/s eta 0:00:01\n","     ---------------------------------------  10.6/10.8 MB 1.6 MB/s eta 0:00:01\n","     ---------------------------------------  10.7/10.8 MB 1.6 MB/s eta 0:00:01\n","     ---------------------------------------  10.7/10.8 MB 1.6 MB/s eta 0:00:01\n","     ---------------------------------------  10.8/10.8 MB 1.6 MB/s eta 0:00:01\n","     ---------------------------------------- 10.8/10.8 MB 1.6 MB/s eta 0:00:00\n","Installing collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.7.4\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2\n","[notice] To update, run: C:\\Users\\theco\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]}],"source":["pip install faiss-cpu"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6724,"status":"ok","timestamp":1687583472079,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"MTRgkY890AvT","outputId":"0d885534-927a-4bed-bfc1-39169b22d60c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pypdf\n","  Downloading pypdf-3.12.2-py3-none-any.whl (254 kB)\n","                                              0.0/255.0 kB ? eta -:--:--\n","     ----------------                       112.6/255.0 kB 2.2 MB/s eta 0:00:01\n","     -------------------------------------- 255.0/255.0 kB 2.6 MB/s eta 0:00:00\n","Installing collected packages: pypdf\n","Successfully installed pypdf-3.12.2\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2\n","[notice] To update, run: C:\\Users\\theco\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install pypdf"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7034,"status":"ok","timestamp":1687583547505,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"rcycykgF0SiL","outputId":"0e72a48e-cab2-4fdf-a87a-3e4653864f86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai\n","  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n","                                              0.0/73.6 kB ? eta -:--:--\n","     --------------------------------------   71.7/73.6 kB 2.0 MB/s eta 0:00:01\n","     ---------------------------------------- 73.6/73.6 kB 1.3 MB/s eta 0:00:00\n","Requirement already satisfied: requests>=2.20 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.31.0)\n","Collecting tqdm (from openai)\n","  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n","Requirement already satisfied: aiohttp in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (3.8.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (2.0.3)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.20->openai) (2023.5.7)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->openai) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->openai) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->openai) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->openai) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->openai) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->openai) (1.3.1)\n","Requirement already satisfied: colorama in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->openai) (0.4.6)\n","Installing collected packages: tqdm, openai\n","Successfully installed openai-0.27.8 tqdm-4.65.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2\n","[notice] To update, run: C:\\Users\\theco\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install openai"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6874,"status":"ok","timestamp":1687583601661,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"FPR71teO0git","outputId":"8b00fe6f-466b-4972-cec1-61770e6a339d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tiktoken"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2\n","[notice] To update, run: C:\\Users\\theco\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["\n","  Downloading tiktoken-0.4.0-cp311-cp311-win_amd64.whl (635 kB)\n","                                              0.0/635.3 kB ? eta -:--:--\n","     ----------                             174.1/635.3 kB 3.5 MB/s eta 0:00:01\n","     -------------------------------------  634.9/635.3 kB 6.6 MB/s eta 0:00:01\n","     -------------------------------------- 635.3/635.3 kB 5.0 MB/s eta 0:00:00\n","Collecting regex>=2022.1.18 (from tiktoken)\n","  Downloading regex-2023.6.3-cp311-cp311-win_amd64.whl (268 kB)\n","                                              0.0/268.0 kB ? eta -:--:--\n","     ------------------------------------  266.2/268.0 kB 16.0 MB/s eta 0:00:01\n","     -------------------------------------- 268.0/268.0 kB 5.5 MB/s eta 0:00:00\n","Requirement already satisfied: requests>=2.26.0 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2.0.3)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\theco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n","Installing collected packages: regex, tiktoken\n","Successfully installed regex-2023.6.3 tiktoken-0.4.0\n"]}],"source":["!pip install tiktoken"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20096,"status":"ok","timestamp":1687583849967,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"nzQ3vMwXyS3M","outputId":"110b1748-f6cf-4dee-ab7f-9af8e76587f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["7: MLOps systems is designing for fluctuating demand, especially in relation to the process of ML training [7]. This stems from potentially voluminous and varying data [10], which makes it difficult to precisely estimate the necessary infrastructure resources (CPU, RAM, and GPU) and requires a high lev\n","0: Machine Learning Operations (MLOps):  Overview, Definition, and Architecture Dominik Kreuzberger  KIT  Germany dominik.kreuzberger@alumni.kit.edu Niklas Kühl  KIT  Germany  kuehl@kit.edu  Sebastian Hirschl   IBM†   Germany   sebastian.hirschl@de.ibm.comABSTRACT The final goal of all industrial machi\n","9: MLOps Kreuzberger, Kühl, and Hirschl  \n"," [41] Damian A. Tamburri. 2020. Sustainable MLOps: Trends and Challenges. Proc. - 2020 22nd Int. Symp. Symb. Numer. Algorithms Sci. Comput. SYNASC 2020 (2020), 17–23. DOI:https://doi.org/10.1109/SYNASC51798.2020.00015 [42] Chandrasekar Vuppalapati, Anitha Ilapa\n"]},{"data":{"text/plain":["[Document(page_content='Machine Learning Operations (MLOps):  Overview, Definition, and Architecture Dominik Kreuzberger  KIT  Germany dominik.kreuzberger@alumni.kit.edu Niklas Kühl  KIT  Germany  kuehl@kit.edu  Sebastian Hirschl   IBM†   Germany   sebastian.hirschl@de.ibm.comABSTRACT The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies. KEYWORDS CI/CD, DevOps, Machine Learning, MLOps, Operations, Workflow Orchestration 1 Introduction  Machine Learning (ML) has become an important technique to leverage the potential of data and allows businesses to be more innovative [1], efficient [13], and sustainable [22]. However, the success of many productive ML applications in real-world settings falls short of expectations [21]. A large number of ML projects fail—with many ML proofs of concept never progressing as far as production [30]. From a research perspective, this does not come as a surprise as the ML community has focused extensively on the building of ML models, but not on (a) building production-ready ML products and (b) providing the necessary coordination of the resulting, often complex ML system components and infrastructure, including the roles required to automate and operate an ML system in a real-world setting [35]. For instance, in many industrial applications, data scientists still manage ML workflows manually  † This paper does not represent an official IBM statement to a great extent, resulting in many issues during the operations of the respective ML solution [26].  To address these issues, the goal of this work is to examine how manual ML processes can be automated and operationalized so that more ML proofs of concept can be brought into production. In this work, we explore the emerging ML engineering practice “Machine Learning Operations”—MLOps for short—precisely addressing the issue of designing and maintaining productive ML. We take a holistic perspective to gain a common understanding of the involved components, principles, roles, and architectures. While existing research sheds some light on various specific aspects of MLOps, a holistic conceptualization, generalization, and clarification of ML systems design are still missing. Different perspectives and conceptions of the term “MLOps” might lead to misunderstandings and miscommunication, which, in turn, can lead to errors in the overall setup of the entire ML system. Thus, we ask the research question: RQ: What is MLOps? To answer that question, we conduct a mixed-method research endeavor to (a) identify important principles of MLOps, (b) carve out functional core components, (c) highlight the roles necessary to successfully implement MLOps, and (d) derive a general architecture for ML systems design. In combination, these insights result in a definition of MLOps, which contributes to a common understanding of the term and related concepts.  In so doing, we hope to positively impact academic and practical discussions by providing clear guidelines', metadata={'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf', 'page': 0}),\n"," Document(page_content='of MLOps, which contributes to a common understanding of the term and related concepts.  In so doing, we hope to positively impact academic and practical discussions by providing clear guidelines for professionals and researchers alike with precise responsibilities. These insights can assist in allowing more proofs of concept to make it into production by having fewer errors in the system’s design and, finally, enabling more robust predictions in real-world environments. The remainder of this work is structured as follows. We will first elaborate on the necessary foundations and related work in the field. Next, we will give an overview of the utilized methodology, consisting of a literature review, a tool review, and an interview study. We then present the insights derived from the application of the methodology and conceptualize these by providing a unifying definition. We conclude the paper with a short summary, limitations, and outlook.', metadata={'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf', 'page': 0})]"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = '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'\n","\n","# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n","# os.environ['FAISS_NO_AVX2'] = '1'\n","\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","# from langchain.document_loaders import TextLoader\n","\n","# from langchain.document_loaders import TextLoader\n","\n","# loader = TextLoader(\"/content/sample_data/pyspark.pdf\")\n","# documents = loader.load()\n","# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","# docs = text_splitter.split_documents(documents)\n","\n","from langchain.document_loaders import PyPDFLoader\n","\n","#loader = PyPDFLoader(\"/content/sample_data/pyspark.pdf\")\n","loader = PyPDFLoader(\"C:\\\\Users\\\\theco\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf\")\n","pages = loader.load_and_split()\n","\n","faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n","docs = faiss_index.similarity_search(\"what is mlops\", k=3)\n","for doc in docs:\n","    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])\n","\n","\n","# embeddings = OpenAIEmbeddings()\n","# db = FAISS.from_documents(docs, embeddings)\n","\n","# query = \"What did the president say about Ketanji Brown Jackson\"\n","# docs = db.similarity_search(query)\n","\n","# print(docs[0].page_content)\n","pages[0:2]"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1687584533068,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"vZudNSQz4Fqz"},"outputs":[],"source":["from langchain.chains.question_answering import load_qa_chain\n","from langchain.llms import OpenAI"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1687584723520,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"Ca-icubm4wda"},"outputs":[],"source":["OPENAI_API_KEY='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'"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":255,"status":"ok","timestamp":1687584774689,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"_kqsjTW04Rre"},"outputs":[],"source":["llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n","#chain = load_qa_chain(llm, chain_type=\"stuff\")\n","chain = load_qa_chain(llm)\n","query='what is difference between mlops and devops?'"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1687584821952,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"v9awoVEq5LNO","outputId":"d7b85ee6-2eab-48bd-dba9-b2f2df6db7cf"},"outputs":[{"data":{"text/plain":["[Document(page_content='MLOps systems is designing for fluctuating demand, especially in relation to the process of ML training [7]. This stems from potentially voluminous and varying data [10], which makes it difficult to precisely estimate the necessary infrastructure resources (CPU, RAM, and GPU) and requires a high level of flexibility in terms of scalability of the infrastructure [7,26] [δ]. Operational challenges. In productive settings, it is challenging to operate ML manually due to different stacks of software and hardware components and their interplay. Therefore, robust automation is required [7,17]. Also, a constant incoming stream of new data forces retraining capabilities. This is a repetitive task which, again, requires a high level of automation [18] [θ]. These repetitive tasks yield a large number of artifacts that require a strong governance [24,29,40] as well as versioning of data, model, and code to ensure robustness and reproducibility [11,27,29]. Lastly, it is challenging to resolve a potential support request (e.g., by finding the root cause), as many parties and components are involved. Failures can be a combination of ML infrastructure and software [26]. 8 Conclusion With the increase of data availability and analytical capabilities, coupled with the constant pressure to innovate, more machine learning products than ever are being developed. However, only a small number of these proofs of concept progress into deployment and production. Furthermore, the academic space has focused intensively on machine learning model building and benchmarking, but too little on operating complex machine learning systems in real-world scenarios. In the real world, we observe data scientists still managing ML workflows manually to a great extent. The paradigm of Machine Learning Operations (MLOps) addresses these challenges. In this work, we shed more light on MLOps. By conducting a mixed-method study analyzing existing literature and tools, as well as interviewing eight experts from the field, we uncover four main aspects of MLOps: its principles, components, roles, and architecture. From these aspects, we infer a holistic definition. The results support a common understanding of the term MLOps and its associated concepts, and will hopefully assist researchers and professionals in setting up successful ML projects in the future.', metadata={'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf', 'page': 7}),\n"," Document(page_content='Machine Learning Operations (MLOps):  Overview, Definition, and Architecture Dominik Kreuzberger  KIT  Germany dominik.kreuzberger@alumni.kit.edu Niklas Kühl  KIT  Germany  kuehl@kit.edu  Sebastian Hirschl   IBM†   Germany   sebastian.hirschl@de.ibm.comABSTRACT The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies. KEYWORDS CI/CD, DevOps, Machine Learning, MLOps, Operations, Workflow Orchestration 1 Introduction  Machine Learning (ML) has become an important technique to leverage the potential of data and allows businesses to be more innovative [1], efficient [13], and sustainable [22]. However, the success of many productive ML applications in real-world settings falls short of expectations [21]. A large number of ML projects fail—with many ML proofs of concept never progressing as far as production [30]. From a research perspective, this does not come as a surprise as the ML community has focused extensively on the building of ML models, but not on (a) building production-ready ML products and (b) providing the necessary coordination of the resulting, often complex ML system components and infrastructure, including the roles required to automate and operate an ML system in a real-world setting [35]. For instance, in many industrial applications, data scientists still manage ML workflows manually  † This paper does not represent an official IBM statement to a great extent, resulting in many issues during the operations of the respective ML solution [26].  To address these issues, the goal of this work is to examine how manual ML processes can be automated and operationalized so that more ML proofs of concept can be brought into production. In this work, we explore the emerging ML engineering practice “Machine Learning Operations”—MLOps for short—precisely addressing the issue of designing and maintaining productive ML. We take a holistic perspective to gain a common understanding of the involved components, principles, roles, and architectures. While existing research sheds some light on various specific aspects of MLOps, a holistic conceptualization, generalization, and clarification of ML systems design are still missing. Different perspectives and conceptions of the term “MLOps” might lead to misunderstandings and miscommunication, which, in turn, can lead to errors in the overall setup of the entire ML system. Thus, we ask the research question: RQ: What is MLOps? To answer that question, we conduct a mixed-method research endeavor to (a) identify important principles of MLOps, (b) carve out functional core components, (c) highlight the roles necessary to successfully implement MLOps, and (d) derive a general architecture for ML systems design. In combination, these insights result in a definition of MLOps, which contributes to a common understanding of the term and related concepts.  In so doing, we hope to positively impact academic and practical discussions by providing clear guidelines', metadata={'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf', 'page': 0}),\n"," Document(page_content='MLOps Kreuzberger, Kühl, and Hirschl  \\n [41] Damian A. Tamburri. 2020. Sustainable MLOps: Trends and Challenges. Proc. - 2020 22nd Int. Symp. Symb. Numer. Algorithms Sci. Comput. SYNASC 2020 (2020), 17–23. DOI:https://doi.org/10.1109/SYNASC51798.2020.00015 [42] Chandrasekar Vuppalapati, Anitha Ilapakurti, Karthik Chillara, Sharat Kedari, and Vanaja Mamidi. 2020. Automating Tiny ML Intelligent Sensors DevOPS Using Microsoft Azure. Proc. - 2020 IEEE Int. Conf. Big Data, Big Data 2020 (2020), 2375–2384. DOI:https://doi.org/10.1109/BigData50022.2020.9377755 [43] Jane Webster and Richard Watson. 2002. Analyzing the Past to Prepare for the Future: Writing a Literature Review. MIS Q. 26, 2 (2002), xiii–xxiii. DOI:https://doi.org/10.1.1.104.6570 [44] Chaoyu Wu, E. Haihong, and Meina Song. 2020. An Automatic Artificial Intelligence Training Platform Based on Kubernetes. ACM Int. Conf. Proceeding Ser. (2020), 58–62. DOI:https://doi.org/10.1145/3378904.3378921 [45] Geum Seong Yoon, Jungsu Han, Seunghyung Lee, and Jong Won Kim. 2020. DevOps Portal Design for SmartX AI Cluster Employing Cloud-Native Machine Learning Workflows. Springer International Publishing. DOI:https://doi.org/10.1007/978-3-030-39746-3_54 [46] Yue Zhou, Yue Yu, and Bo Ding. 2020. Towards MLOps: A Case Study of ML Pipeline Platform. Proc. - 2020 Int. Conf. Artif. Intell. Comput. Eng. ICAICE 2020 (2020), 494–500. DOI:https://doi.org/10.1109/ICAICE51518.2020.00102', metadata={'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf', 'page': 9})]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["docs"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4540,"status":"ok","timestamp":1687584781840,"user":{"displayName":"Venka Reddy","userId":"10877147519816262591"},"user_tz":420},"id":"JxKl_ppt4LOH","outputId":"2f00c78b-f2fc-4efc-d471-8502cb00f6e9"},"outputs":[{"name":"stdout","output_type":"stream","text":[" MLOps is a specific type of DevOps that focuses on the automation and operationalization of machine learning projects. MLOps includes best practices, sets of concepts, and development culture that are tailored to the specific needs of ML projects. DevOps, on the other hand, is a more general term that refers to the practice of automating and operationalizing software development projects.\n"]}],"source":["answer = chain.run(input_documents=docs, question=query)\n","print(answer)"]},{"cell_type":"markdown","metadata":{},"source":["### Insert into Pinecone Index "]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2\n","[notice] To update, run: C:\\Users\\theco\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install pinecone-client -q"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"text/plain":["1536"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["import openai\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","embeddings = OpenAIEmbeddings()\n","\n","query_result = embeddings.embed_query(\"Hello Pinecone and I attended summit in SF and is great and see again in next year\")\n","len(query_result)\n"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["import pinecone\n","from langchain.vectorstores import Pinecone\n","# initialize pinecone\n","pinecone.init(\n","    api_key=\"9a70fdc8-3994-4d5d-9ad6-379b862ca6f9\",  # find at app.pinecone.io\n","    environment=\"us-west1-gcp-free\"  # next to api key in console\n",")\n","\n","index_name = \"semantic-search-fast\"\n","\n","index = Pinecone.from_documents(pages, OpenAIEmbeddings(), index_name=index_name)\n","#faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())  ## fb --in memory\n"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='MLOps: Overview, Definition, and Architecture Kreuzberger, Kühl, and Hirschl  \\n  Appendix Table 1. List of evaluated technologies  Technology Name Description Sources Open-source examples TensorFlow Extended TensorFlow Extended (TFX) is a configuration framework providing libraries for each of the tasks of an end-to-end ML pipeline. Examples are data validation, data distribution checks, model training, and model serving. [7,10,26,46] [δ, θ]  Airflow Airflow is a task and workflow orchestration tool, which can also be used for ML workflow orchestration. It is also used for orchestrating data engineering jobs. Tasks are executed according to directed acyclic graphs (DAGs). [26,40,41] [α, β, ζ, η] Kubeflow Kubeflow is a Kubernetes-based end-to-end ML platform. Each Kubeflow component is wrapped into a container and orchestrated by Kubernetes. Also, each task of an ML workflow pipeline is handled with one container. [26,35,40,41,46] [α, β, γ, δ, ζ, η, θ]   MLflow MLflow is an ML platform that allows for the management of the ML lifecycle end-to-end. It provides an advanced experiment tracking functionality, a model registry, and model serving component. [11,32,35] [α, γ, ε, ζ, η, θ]   Commercial examples Databricks managed MLflow The Databricks platform offers managed services based on other cloud providers’ infrastructure, e.g., managed MLflow. [26,32,35,40] [α, ζ] Amazon CodePipeline Amazon CodePipeline is a CI/CD automation tool to facilitate the build, test, and delivery steps. It also allows one to schedule and manage the different stages of an ML pipeline. [18] [γ] Amazon SageMaker With SageMaker, Amazon AWS offers an end-to-end ML platform. It provides, out-of-the-box, a feature store, orchestration with SageMaker Pipelines, and model serving with SageMaker endpoints. [7,11,18,24,35] [α, β, γ, ζ, θ]  Azure DevOps Pipelines Azure DevOps Pipelines is a CI/CD automation tool to facilitate the build, test, and delivery steps. It also allows one to schedule and manage the different stages of an ML pipeline. [18,42] [γ, ε] Azure ML Microsoft Azure offers, in combination with Azure DevOps Pipelines and Azure ML, an end-to-end ML platform. [6,24,25,35,42] [α, γ, ε, ζ, η, θ]', metadata={'page': 10.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='MLOps: Overview, Definition, and Architecture Kreuzberger, Kühl, and Hirschl  \\n  Appendix Table 1. List of evaluated technologies  Technology Name Description Sources Open-source examples TensorFlow Extended TensorFlow Extended (TFX) is a configuration framework providing libraries for each of the tasks of an end-to-end ML pipeline. Examples are data validation, data distribution checks, model training, and model serving. [7,10,26,46] [δ, θ]  Airflow Airflow is a task and workflow orchestration tool, which can also be used for ML workflow orchestration. It is also used for orchestrating data engineering jobs. Tasks are executed according to directed acyclic graphs (DAGs). [26,40,41] [α, β, ζ, η] Kubeflow Kubeflow is a Kubernetes-based end-to-end ML platform. Each Kubeflow component is wrapped into a container and orchestrated by Kubernetes. Also, each task of an ML workflow pipeline is handled with one container. [26,35,40,41,46] [α, β, γ, δ, ζ, η, θ]   MLflow MLflow is an ML platform that allows for the management of the ML lifecycle end-to-end. It provides an advanced experiment tracking functionality, a model registry, and model serving component. [11,32,35] [α, γ, ε, ζ, η, θ]   Commercial examples Databricks managed MLflow The Databricks platform offers managed services based on other cloud providers’ infrastructure, e.g., managed MLflow. [26,32,35,40] [α, ζ] Amazon CodePipeline Amazon CodePipeline is a CI/CD automation tool to facilitate the build, test, and delivery steps. It also allows one to schedule and manage the different stages of an ML pipeline. [18] [γ] Amazon SageMaker With SageMaker, Amazon AWS offers an end-to-end ML platform. It provides, out-of-the-box, a feature store, orchestration with SageMaker Pipelines, and model serving with SageMaker endpoints. [7,11,18,24,35] [α, β, γ, ζ, θ]  Azure DevOps Pipelines Azure DevOps Pipelines is a CI/CD automation tool to facilitate the build, test, and delivery steps. It also allows one to schedule and manage the different stages of an ML pipeline. [18,42] [γ, ε] Azure ML Microsoft Azure offers, in combination with Azure DevOps Pipelines and Azure ML, an end-to-end ML platform. [6,24,25,35,42] [α, γ, ε, ζ, η, θ]', metadata={'page': 10.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='MLOps: Overview, Definition, and Architecture Kreuzberger, Kühl, and Hirschl  \\n  Appendix Table 1. List of evaluated technologies  Technology Name Description Sources Open-source examples TensorFlow Extended TensorFlow Extended (TFX) is a configuration framework providing libraries for each of the tasks of an end-to-end ML pipeline. Examples are data validation, data distribution checks, model training, and model serving. [7,10,26,46] [δ, θ]  Airflow Airflow is a task and workflow orchestration tool, which can also be used for ML workflow orchestration. It is also used for orchestrating data engineering jobs. Tasks are executed according to directed acyclic graphs (DAGs). [26,40,41] [α, β, ζ, η] Kubeflow Kubeflow is a Kubernetes-based end-to-end ML platform. Each Kubeflow component is wrapped into a container and orchestrated by Kubernetes. Also, each task of an ML workflow pipeline is handled with one container. [26,35,40,41,46] [α, β, γ, δ, ζ, η, θ]   MLflow MLflow is an ML platform that allows for the management of the ML lifecycle end-to-end. It provides an advanced experiment tracking functionality, a model registry, and model serving component. [11,32,35] [α, γ, ε, ζ, η, θ]   Commercial examples Databricks managed MLflow The Databricks platform offers managed services based on other cloud providers’ infrastructure, e.g., managed MLflow. [26,32,35,40] [α, ζ] Amazon CodePipeline Amazon CodePipeline is a CI/CD automation tool to facilitate the build, test, and delivery steps. It also allows one to schedule and manage the different stages of an ML pipeline. [18] [γ] Amazon SageMaker With SageMaker, Amazon AWS offers an end-to-end ML platform. It provides, out-of-the-box, a feature store, orchestration with SageMaker Pipelines, and model serving with SageMaker endpoints. [7,11,18,24,35] [α, β, γ, ζ, θ]  Azure DevOps Pipelines Azure DevOps Pipelines is a CI/CD automation tool to facilitate the build, test, and delivery steps. It also allows one to schedule and manage the different stages of an ML pipeline. [18,42] [γ, ε] Azure ML Microsoft Azure offers, in combination with Azure DevOps Pipelines and Azure ML, an end-to-end ML platform. [6,24,25,35,42] [α, γ, ε, ζ, η, θ]', metadata={'page': 10.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='C2 Source Code Repository (P4, P5). The source code repository ensures code storing and versioning. It allows multiple developers to commit and merge their code [17,25,42,44,46] [α, β, γ, ζ, θ]. Examples include Bitbucket [11] [ζ], GitLab [11,17] [ζ], GitHub [25] [ζ ,η], and Gitea [46]. C3 Workflow Orchestration Component (P2, P3, P6). The workflow orchestration component offers task orchestration of an ML workflow via directed acyclic graphs (DAGs). These graphs represent execution order and artifact usage of single steps of the workflow [26,32,35,40,41,46] [α, β, γ, δ, ε, ζ, η]. Examples include Apache Airflow [α, ζ], Kubeflow Pipelines [ζ], Luigi [ζ], AWS SageMaker Pipelines [β], and Azure Pipelines [ε]. C4 Feature Store System (P3, P4). A feature store system ensures central storage of commonly used features. It has two databases configured: One database as an offline feature store to serve features with normal latency for experimentation, and one database as an online store to serve features with low latency for predictions in production [10,14] [α, β, ζ, ε, θ]. Examples include Google Feast [ζ], Amazon AWS Feature Store [β, ζ], Tecton.ai and Hopswork.ai [ζ]. This is where most of the data for training ML models will come from. Moreover, data can also come directly from any kind of data store. C5 Model Training Infrastructure (P6). The model training infrastructure provides the foundational computation resources, e.g., CPUs, RAM, and GPUs. The provided infrastructure can be either distributed or non-distributed. In general, a scalable and distributed infrastructure is recommended [7,10,24–26,29,40,45,46] [δ, ζ, η, θ]. Examples include local machines (not scalable) or cloud computation [7] [η, θ], as well as non-distributed or distributed computation (several worker nodes) [25,27]. Frameworks supporting computation are Kubernetes [η, θ] and Red Hat OpenShift [γ]. C6 Model Registry (P3, P4). The model registry stores centrally the trained ML models together with their metadata. It has two main functionalities: storing the ML artifact and storing the ML metadata (see C7) [4,6,14,17,26,27] [α, β, γ, ε, ζ, η, θ]. Advanced storage examples include MLflow [α, η, ζ], AWS SageMaker Model Registry [ζ], Microsoft Azure ML Model Registry [ζ], and Neptune.ai [α]. Simple storage examples include Microsoft Azure Storage, Google Cloud Storage, and Amazon AWS S3 [17]. C7 ML Metadata Stores (P4, P7). ML metadata stores allow for the tracking of various kinds of metadata, e.g., for each orchestrated ML workflow pipeline task. Another metadata store can be configured within the model registry for tracking and logging the metadata of each training job (e.g., training date and time, duration, etc.), including the model specific metadata—e.g., used parameters and the resulting performance metrics, model lineage: data and code used [14,25–27,32] [α, β, δ, ζ, θ]. Examples include orchestrators with built-in metadata stores tracking each step of experiment pipelines [α] such as Kubeflow Pipelines [α,ζ], AWS SageMaker Pipelines [α,ζ], Azure ML, and IBM Watson Studio [γ]. MLflow provides an advanced metadata store in combination with the model registry [32,35].  C8 Model Serving Component (P1). The model serving component can be configured for different purposes. Examples are online inference for real-time predictions or batch inference for predictions using large volumes of input data. The serving can be provided, e.g., via a REST API. As a foundational infrastructure layer, a scalable and distributed model serving infrastructure is recommended [7,11,25,40,45,46] [α, β, δ, ζ, η, θ]. One example of a model serving component configuration is the use of Kubernetes and Docker technology to containerize the ML model, and leveraging a Python web application framework like Flask [17] with an API for serving [α]. Other Kubernetes supported frameworks are KServing of Kubeflow [α], TensorFlow Serving, and Seldion.io serving [40]. Inferencing could also be', metadata={'page': 3.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='C2 Source Code Repository (P4, P5). The source code repository ensures code storing and versioning. It allows multiple developers to commit and merge their code [17,25,42,44,46] [α, β, γ, ζ, θ]. Examples include Bitbucket [11] [ζ], GitLab [11,17] [ζ], GitHub [25] [ζ ,η], and Gitea [46]. C3 Workflow Orchestration Component (P2, P3, P6). The workflow orchestration component offers task orchestration of an ML workflow via directed acyclic graphs (DAGs). These graphs represent execution order and artifact usage of single steps of the workflow [26,32,35,40,41,46] [α, β, γ, δ, ε, ζ, η]. Examples include Apache Airflow [α, ζ], Kubeflow Pipelines [ζ], Luigi [ζ], AWS SageMaker Pipelines [β], and Azure Pipelines [ε]. C4 Feature Store System (P3, P4). A feature store system ensures central storage of commonly used features. It has two databases configured: One database as an offline feature store to serve features with normal latency for experimentation, and one database as an online store to serve features with low latency for predictions in production [10,14] [α, β, ζ, ε, θ]. Examples include Google Feast [ζ], Amazon AWS Feature Store [β, ζ], Tecton.ai and Hopswork.ai [ζ]. This is where most of the data for training ML models will come from. Moreover, data can also come directly from any kind of data store. C5 Model Training Infrastructure (P6). The model training infrastructure provides the foundational computation resources, e.g., CPUs, RAM, and GPUs. The provided infrastructure can be either distributed or non-distributed. In general, a scalable and distributed infrastructure is recommended [7,10,24–26,29,40,45,46] [δ, ζ, η, θ]. Examples include local machines (not scalable) or cloud computation [7] [η, θ], as well as non-distributed or distributed computation (several worker nodes) [25,27]. Frameworks supporting computation are Kubernetes [η, θ] and Red Hat OpenShift [γ]. C6 Model Registry (P3, P4). The model registry stores centrally the trained ML models together with their metadata. It has two main functionalities: storing the ML artifact and storing the ML metadata (see C7) [4,6,14,17,26,27] [α, β, γ, ε, ζ, η, θ]. Advanced storage examples include MLflow [α, η, ζ], AWS SageMaker Model Registry [ζ], Microsoft Azure ML Model Registry [ζ], and Neptune.ai [α]. Simple storage examples include Microsoft Azure Storage, Google Cloud Storage, and Amazon AWS S3 [17]. C7 ML Metadata Stores (P4, P7). ML metadata stores allow for the tracking of various kinds of metadata, e.g., for each orchestrated ML workflow pipeline task. Another metadata store can be configured within the model registry for tracking and logging the metadata of each training job (e.g., training date and time, duration, etc.), including the model specific metadata—e.g., used parameters and the resulting performance metrics, model lineage: data and code used [14,25–27,32] [α, β, δ, ζ, θ]. Examples include orchestrators with built-in metadata stores tracking each step of experiment pipelines [α] such as Kubeflow Pipelines [α,ζ], AWS SageMaker Pipelines [α,ζ], Azure ML, and IBM Watson Studio [γ]. MLflow provides an advanced metadata store in combination with the model registry [32,35].  C8 Model Serving Component (P1). The model serving component can be configured for different purposes. Examples are online inference for real-time predictions or batch inference for predictions using large volumes of input data. The serving can be provided, e.g., via a REST API. As a foundational infrastructure layer, a scalable and distributed model serving infrastructure is recommended [7,11,25,40,45,46] [α, β, δ, ζ, η, θ]. One example of a model serving component configuration is the use of Kubernetes and Docker technology to containerize the ML model, and leveraging a Python web application framework like Flask [17] with an API for serving [α]. Other Kubernetes supported frameworks are KServing of Kubeflow [α], TensorFlow Serving, and Seldion.io serving [40]. Inferencing could also be', metadata={'page': 3.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='C2 Source Code Repository (P4, P5). The source code repository ensures code storing and versioning. It allows multiple developers to commit and merge their code [17,25,42,44,46] [α, β, γ, ζ, θ]. Examples include Bitbucket [11] [ζ], GitLab [11,17] [ζ], GitHub [25] [ζ ,η], and Gitea [46]. C3 Workflow Orchestration Component (P2, P3, P6). The workflow orchestration component offers task orchestration of an ML workflow via directed acyclic graphs (DAGs). These graphs represent execution order and artifact usage of single steps of the workflow [26,32,35,40,41,46] [α, β, γ, δ, ε, ζ, η]. Examples include Apache Airflow [α, ζ], Kubeflow Pipelines [ζ], Luigi [ζ], AWS SageMaker Pipelines [β], and Azure Pipelines [ε]. C4 Feature Store System (P3, P4). A feature store system ensures central storage of commonly used features. It has two databases configured: One database as an offline feature store to serve features with normal latency for experimentation, and one database as an online store to serve features with low latency for predictions in production [10,14] [α, β, ζ, ε, θ]. Examples include Google Feast [ζ], Amazon AWS Feature Store [β, ζ], Tecton.ai and Hopswork.ai [ζ]. This is where most of the data for training ML models will come from. Moreover, data can also come directly from any kind of data store. C5 Model Training Infrastructure (P6). The model training infrastructure provides the foundational computation resources, e.g., CPUs, RAM, and GPUs. The provided infrastructure can be either distributed or non-distributed. In general, a scalable and distributed infrastructure is recommended [7,10,24–26,29,40,45,46] [δ, ζ, η, θ]. Examples include local machines (not scalable) or cloud computation [7] [η, θ], as well as non-distributed or distributed computation (several worker nodes) [25,27]. Frameworks supporting computation are Kubernetes [η, θ] and Red Hat OpenShift [γ]. C6 Model Registry (P3, P4). The model registry stores centrally the trained ML models together with their metadata. It has two main functionalities: storing the ML artifact and storing the ML metadata (see C7) [4,6,14,17,26,27] [α, β, γ, ε, ζ, η, θ]. Advanced storage examples include MLflow [α, η, ζ], AWS SageMaker Model Registry [ζ], Microsoft Azure ML Model Registry [ζ], and Neptune.ai [α]. Simple storage examples include Microsoft Azure Storage, Google Cloud Storage, and Amazon AWS S3 [17]. C7 ML Metadata Stores (P4, P7). ML metadata stores allow for the tracking of various kinds of metadata, e.g., for each orchestrated ML workflow pipeline task. Another metadata store can be configured within the model registry for tracking and logging the metadata of each training job (e.g., training date and time, duration, etc.), including the model specific metadata—e.g., used parameters and the resulting performance metrics, model lineage: data and code used [14,25–27,32] [α, β, δ, ζ, θ]. Examples include orchestrators with built-in metadata stores tracking each step of experiment pipelines [α] such as Kubeflow Pipelines [α,ζ], AWS SageMaker Pipelines [α,ζ], Azure ML, and IBM Watson Studio [γ]. MLflow provides an advanced metadata store in combination with the model registry [32,35].  C8 Model Serving Component (P1). The model serving component can be configured for different purposes. Examples are online inference for real-time predictions or batch inference for predictions using large volumes of input data. The serving can be provided, e.g., via a REST API. As a foundational infrastructure layer, a scalable and distributed model serving infrastructure is recommended [7,11,25,40,45,46] [α, β, δ, ζ, η, θ]. One example of a model serving component configuration is the use of Kubernetes and Docker technology to containerize the ML model, and leveraging a Python web application framework like Flask [17] with an API for serving [α]. Other Kubernetes supported frameworks are KServing of Kubeflow [α], TensorFlow Serving, and Seldion.io serving [40]. Inferencing could also be', metadata={'page': 3.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='framework like Flask [17] with an API for serving [α]. Other Kubernetes supported frameworks are KServing of Kubeflow [α], TensorFlow Serving, and Seldion.io serving [40]. Inferencing could also be realized with Apache Spark for batch predictions [θ]. Examples of cloud services include Microsoft Azure ML REST API [ε], AWS SageMaker Endpoints [α, β], IBM Watson Studio [γ], and Google Vertex AI prediction service [δ]. C9 Monitoring Component (P8, P9). The monitoring component takes care of the continuous monitoring of the model serving performance (e.g., prediction accuracy). Additionally, monitoring of the ML infrastructure, CI/CD, and orchestration are required [7,10,17,26,29,36,46] [α, ζ, η, θ]. Examples include Prometheus with Grafana [η, ζ], ELK stack (Elasticsearch, Logstash, and Kibana) [α, η, ζ], and simply TensorBoard [θ]. Examples with built-in monitoring capabilities are Kubeflow [θ], MLflow [η], and AWS SageMaker model monitor or cloud watch [ζ]. 4.3 Roles After describing the principles and their resulting instantiation of components, we identify necessary roles in order to realize MLOps in the following. MLOps is an interdisciplinary group process, and the interplay of different roles is crucial to design, manage, automate, and operate an ML system in production. In the following, every role, its purpose, and related tasks are briefly described: R1 Business Stakeholder (similar roles: Product Owner, Project Manager). The business stakeholder defines the business goal to be achieved with ML and takes care of the communication side of the business, e.g., presenting the return on investment (ROI) generated with an ML product [17,24,26] [α, β, δ, θ].  R2D2Solution Architect (similar role: IT Architect). The solution architect designs the architecture and defines the technologies to be used, following a thorough evaluation [17,27] [α, ζ].  R3 Data Scientist (similar roles: ML Specialist, ML Developer). The data scientist translates the business problem into an ML problem and takes care of the model engineering, including the selection of the best-performing algorithm and hyperparameters [7,14,26,29] [α, β, γ, δ, ε, ζ, η, θ].  R4 Data Engineer (similar role: DataOps Engineer). The data engineer builds up and manages data and feature engineering pipelines. Moreover, this role ensures proper data ingestion to the databases of the feature store system [14,29,41] [α, β, γ, δ, ε, ζ, η, θ].', metadata={'page': 3.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='framework like Flask [17] with an API for serving [α]. Other Kubernetes supported frameworks are KServing of Kubeflow [α], TensorFlow Serving, and Seldion.io serving [40]. Inferencing could also be realized with Apache Spark for batch predictions [θ]. Examples of cloud services include Microsoft Azure ML REST API [ε], AWS SageMaker Endpoints [α, β], IBM Watson Studio [γ], and Google Vertex AI prediction service [δ]. C9 Monitoring Component (P8, P9). The monitoring component takes care of the continuous monitoring of the model serving performance (e.g., prediction accuracy). Additionally, monitoring of the ML infrastructure, CI/CD, and orchestration are required [7,10,17,26,29,36,46] [α, ζ, η, θ]. Examples include Prometheus with Grafana [η, ζ], ELK stack (Elasticsearch, Logstash, and Kibana) [α, η, ζ], and simply TensorBoard [θ]. Examples with built-in monitoring capabilities are Kubeflow [θ], MLflow [η], and AWS SageMaker model monitor or cloud watch [ζ]. 4.3 Roles After describing the principles and their resulting instantiation of components, we identify necessary roles in order to realize MLOps in the following. MLOps is an interdisciplinary group process, and the interplay of different roles is crucial to design, manage, automate, and operate an ML system in production. In the following, every role, its purpose, and related tasks are briefly described: R1 Business Stakeholder (similar roles: Product Owner, Project Manager). The business stakeholder defines the business goal to be achieved with ML and takes care of the communication side of the business, e.g., presenting the return on investment (ROI) generated with an ML product [17,24,26] [α, β, δ, θ].  R2D2Solution Architect (similar role: IT Architect). The solution architect designs the architecture and defines the technologies to be used, following a thorough evaluation [17,27] [α, ζ].  R3 Data Scientist (similar roles: ML Specialist, ML Developer). The data scientist translates the business problem into an ML problem and takes care of the model engineering, including the selection of the best-performing algorithm and hyperparameters [7,14,26,29] [α, β, γ, δ, ε, ζ, η, θ].  R4 Data Engineer (similar role: DataOps Engineer). The data engineer builds up and manages data and feature engineering pipelines. Moreover, this role ensures proper data ingestion to the databases of the feature store system [14,29,41] [α, β, γ, δ, ε, ζ, η, θ].', metadata={'page': 3.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='framework like Flask [17] with an API for serving [α]. Other Kubernetes supported frameworks are KServing of Kubeflow [α], TensorFlow Serving, and Seldion.io serving [40]. Inferencing could also be realized with Apache Spark for batch predictions [θ]. Examples of cloud services include Microsoft Azure ML REST API [ε], AWS SageMaker Endpoints [α, β], IBM Watson Studio [γ], and Google Vertex AI prediction service [δ]. C9 Monitoring Component (P8, P9). The monitoring component takes care of the continuous monitoring of the model serving performance (e.g., prediction accuracy). Additionally, monitoring of the ML infrastructure, CI/CD, and orchestration are required [7,10,17,26,29,36,46] [α, ζ, η, θ]. Examples include Prometheus with Grafana [η, ζ], ELK stack (Elasticsearch, Logstash, and Kibana) [α, η, ζ], and simply TensorBoard [θ]. Examples with built-in monitoring capabilities are Kubeflow [θ], MLflow [η], and AWS SageMaker model monitor or cloud watch [ζ]. 4.3 Roles After describing the principles and their resulting instantiation of components, we identify necessary roles in order to realize MLOps in the following. MLOps is an interdisciplinary group process, and the interplay of different roles is crucial to design, manage, automate, and operate an ML system in production. In the following, every role, its purpose, and related tasks are briefly described: R1 Business Stakeholder (similar roles: Product Owner, Project Manager). The business stakeholder defines the business goal to be achieved with ML and takes care of the communication side of the business, e.g., presenting the return on investment (ROI) generated with an ML product [17,24,26] [α, β, δ, θ].  R2D2Solution Architect (similar role: IT Architect). The solution architect designs the architecture and defines the technologies to be used, following a thorough evaluation [17,27] [α, ζ].  R3 Data Scientist (similar roles: ML Specialist, ML Developer). The data scientist translates the business problem into an ML problem and takes care of the model engineering, including the selection of the best-performing algorithm and hyperparameters [7,14,26,29] [α, β, γ, δ, ε, ζ, η, θ].  R4 Data Engineer (similar role: DataOps Engineer). The data engineer builds up and manages data and feature engineering pipelines. Moreover, this role ensures proper data ingestion to the databases of the feature store system [14,29,41] [α, β, γ, δ, ε, ζ, η, θ].', metadata={'page': 3.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'}),\n"," Document(page_content='or data from any cloud storage. (9) The data will be extracted from the data sources. (10) The data preprocessing begins with data transformation and cleaning tasks. The transformation rule artifact defined in the requirement gathering stage serves as input for this task, and the main aim of this task is to bring the data into a usable format. These transformation rules are continuously improved based on the feedback.  Data Scientist(ML model development)Data Engineer(data management, data pipeline management) Backend Engineer(ML infrastructure management)DevOps Engineer(Software engineer with DevOps skills,ML workflow pipeline orchestration,CI/CD pipeline management,monitoring) ML Engineer /MLOps Engineer(cross-functional managementof ML environment and assets:ML infrastructure,ML models,ML workflow pipelines,data Ingestion,monitoring)', metadata={'page': 4.0, 'source': 'C:\\\\Users\\\\theco\\\\OneDrive\\\\Documents\\\\git_repo\\\\mlops\\\\data\\\\mlops.pdf'})]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["index.similarity_search(\"pyspark\",k=10)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMrKNraCs/OXYYiNAU3VUoz","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
